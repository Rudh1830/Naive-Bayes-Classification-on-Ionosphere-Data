# Naive Bayes Classification on Ionosphere Dataset

This project applies **Gaussian Naive Bayes** to classify radar signal returns from the
**Ionosphere dataset** as *good* or *bad*. The dataset is widely used to benchmark
classification algorithms on high-dimensional numerical data.

The complete machine learning pipeline â€” preprocessing, training, prediction, and evaluation â€”
is implemented in Python using scikit-learn.

---

## ðŸ“Š Dataset Information
- **Source**: UCI Machine Learning Repository (Ionosphere)
- **Number of Samples**: 351
- **Number of Features**: 34
  - 32 continuous numeric features
  - 2 boolean features (converted to 0/1)
- **Target Column**: `column_ai`
- **Target Classes**:
  - `g` â†’ Good radar return
  - `b` â†’ Bad radar return


---

## ðŸŽ¯ Objective

To build and evaluate a **Naive Bayes classifier** that can accurately distinguish
between good and bad radar signal returns.

---

## âš™ï¸ Technologies Used

- Python 3.x
- NumPy
- Pandas
- Scikit-learn
- Matplotlib
- Seaborn

---

## ðŸ§ª Workflow Implemented

1. Data loading and inspection
2. Featureâ€“target separation
3. Label encoding of target variable
4. Trainâ€“test split (70/30)
5. Model training using **Gaussian Naive Bayes**
6. Prediction on test data
7. Model evaluation
   - Accuracy score
   - Classification report
   - Confusion matrix visualization

---

## ðŸ§  Why Gaussian Naive Bayes?

- All features are continuous â†’ GaussianNB is appropriate
- Works well with high-dimensional data
- Fast training and inference
- Strong baseline model for comparison

---

## ðŸ“ˆ Model Evaluation

The model is evaluated using:
- Accuracy
- Precision, Recall, F1-score
- Confusion Matrix

> Results may vary slightly due to random train-test split.

---

ðŸ“Œ Sample Code Snippet
---
from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

---
Processing Steps
---
1. Featureâ€“target separation
2. Conversion of boolean features (`column_a`, `column_b`) to integers
3. Label encoding of target variable (`g`, `b`)
4. Trainâ€“test split (70/30)
5. Model training using Gaussian Naive Bayes

---
ðŸš€ Future Improvements

Compare with Logistic Regression, SVM, and Random Forest

Apply cross-validation

Feature selection

Hyperparameter tuning

Deploy using Streamlit or Flask

---
Tags
---

Machine Learning Classification Naive Bayes Ionosphere Dataset
Scikit-learn Python Data Science
